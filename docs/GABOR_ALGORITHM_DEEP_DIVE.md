# Gabor Filter Algorithm - Deep Dive

## Core Mathematical Foundation

### What is a Gabor Filter?

A **Gabor filter** is the product of a **Gaussian envelope** and a **harmonic carrier** (sinusoidal wave):

$$
G(x, y; \theta, f, \phi, \sigma_x, \sigma_y) = \exp\left(-\frac{x'^2}{2\sigma_x^2} - \frac{y'^2}{2\sigma_y^2}\right) \cdot \cos(2\pi f x' + \phi)
$$

Where:
- **Rotated coordinates**: $x' = x\cos\theta + y\sin\theta$, $y' = -x\sin\theta + y\cos\theta$
- $\theta$ = **orientation** (angle of edge detection, 0 to 2Ï€)
- $f$ = **spatial frequency** (cycles per pixel, controls stripe spacing)
- $\phi$ = **phase** (shift of the wave, 0 to 2Ï€)
- $\sigma_x, \sigma_y$ = **Gaussian spreads** (anisotropic, controls receptive field shape)

**Biological motivation**: V1 simple cells in mammalian visual cortex behave like Gabor filters - they respond to oriented edges at specific frequencies.

---

## Algorithm Implementation

### Step-by-Step Kernel Building (`_build_kernels`)

```python
def _build_kernels(self, device: torch.device) -> torch.Tensor:
    # 1. MAP UNCONSTRAINED PARAMETERS TO VALID RANGES
    theta = self.theta_u % (2 * math.pi)                              # [0, 2Ï€]
    phase = self.phase_u % (2 * math.pi)                              # [0, 2Ï€]
    freq = fmin + sigmoid(self.freq_u) * (fmax - fmin)                # [fmin, fmax]
    sigx = softplus(self.sigx_u) + smin                               # [smin, âˆ)
    sigy = softplus(self.sigy_u) + smin                               # [smin, âˆ)
    
    # 2. CREATE SPATIAL COORDINATE MESH
    # For kernel_size=31: x,y âˆˆ [-15, 15] (centered at 0)
    x, y = meshgrid(linspace(-r, r, k), ...)                         # [31, 31]
    
    # 3. ROTATE COORDINATES BY Î¸ (per filter)
    cos_t = cos(theta).view(-1, 1, 1)                                 # [N, 1, 1]
    sin_t = sin(theta).view(-1, 1, 1)
    xp = cos_t * x + sin_t * y                                        # [N, 31, 31]
    yp = -sin_t * x + cos_t * y
    
    # 4. GAUSSIAN ENVELOPE (anisotropic)
    env = exp(-0.5 * ((xp/Ïƒx)Â² + (yp/Ïƒy)Â²))                          # [N, 31, 31]
    
    # 5. HARMONIC CARRIER (sinusoidal grating)
    carrier = cos(2Ï€ * f * xp + Ï†)                                    # [N, 31, 31]
    
    # 6. MULTIPLY: Gabor = Gaussian Ã— Carrier
    k = env * carrier                                                 # [N, 31, 31]
    
    # 7. NORMALIZE: Zero-mean, unit L2 norm
    k = k - k.mean(dim=(1,2), keepdim=True)                          # remove DC
    k = k / (k.norm(dim=(1,2)) + Îµ)                                  # unit energy
    
    # 8. SCALE BY LEARNABLE AMPLITUDE
    # v1: k *= gain * gate
    # v2: k *= gate only (gain=1 fixed)
    
    return k  # [N, kernel_size, kernel_size]
```

---

## Key Differences: v1 vs v2

### Version 1 (`gabor_cnn.py`) - With Learnable Gain

| Component | Implementation | Rationale |
|-----------|----------------|-----------|
| **Amplitude** | `gain_u` (learnable) â†’ `tanh(gain_u)` âˆˆ [-1, 1] | Each filter learns its own strength |
| **Gating** | `gate_u` (learnable) â†’ `sigmoid(gate_u)` âˆˆ [0, 1] | Controls filter activation (quantity) |
| **Final scale** | `k *= gain * gate` | Combined amplitude and gating |
| **Sparsity loss** | `sigmoid(gate_u).abs().mean()` | L1 penalty on gates |
| **Post-processing** | Learnable affine (depthwise 1Ã—1 conv) + SiLU | Preserves per-channel amplitude |

**Parameters per filter**: Î¸, f, Ï†, Ïƒx, Ïƒy, **gain**, gate = **7 params**

### Version 2 (`gabor_cnn_2.py`) - Fixed Gain

| Component | Implementation | Rationale |
|-----------|----------------|-----------|
| **Amplitude** | Fixed to 1.0 (removed `gain_u`) | Simplification - gate controls amplitude |
| **Gating** | `gate_u` (learnable) â†’ `sigmoid(gate_u)` âˆˆ [0, 1] | Single amplitude control |
| **Final scale** | `k *= gate` | Only gating |
| **Sparsity loss** | `(g * (1-g)).mean() * 4` | **Entropy penalty** - encourages discrete 0/1 |
| **Post-processing** | SiLU only (no learnable affine) | Simpler, shared nonlinearity |

**Parameters per filter**: Î¸, f, Ï†, Ïƒx, Ïƒy, gate = **6 params** (14% reduction)

---

## Learnable Properties - Current State

### âœ… Already Learnable (Both Versions)

| Property | Parameter | Range | Mapping | What it Controls |
|----------|-----------|-------|---------|------------------|
| **Orientation** | `theta_u` | â„ | `% (2Ï€)` | Edge direction (0Â°=horizontal, 90Â°=vertical) |
| **Frequency** | `freq_u` | â„ | `fmin + sigmoid(Â·)*(fmax-fmin)` | Stripe spacing (0.05-0.25 cycles/pixel) |
| **Phase** | `phase_u` | â„ | `% (2Ï€)` | Wave shift (0=cosine, Ï€/2=sine, Ï€=-cosine) |
| **Sigma X** | `sigx_u` | â„ | `softplus(Â·) + smin` | Horizontal spread (min 3.0 pixels) |
| **Sigma Y** | `sigy_u` | â„ | `softplus(Â·) + smin` | Vertical spread (min 3.0 pixels) |
| **Gate** | `gate_u` | â„ | `sigmoid(Â·)` | Filter activation strength [0,1] |
| **Gain** (v1 only) | `gain_u` | â„ | `tanh(Â·)` | Amplitude multiplier [-1,1] |

### ğŸ” What Each Property Does Visually

**Î¸ (Orientation)**: Rotates the stripes
```
Î¸=0Â°:     Î¸=45Â°:    Î¸=90Â°:
â”€â”€â”€â”€â”€     â•²â•²â•²â•²â•²    â”‚â”‚â”‚â”‚â”‚
â”€â”€â”€â”€â”€     â•²â•²â•²â•²â•²    â”‚â”‚â”‚â”‚â”‚
â”€â”€â”€â”€â”€     â•²â•²â•²â•²â•²    â”‚â”‚â”‚â”‚â”‚
```

**f (Frequency)**: Controls stripe density
```
f=0.1:    f=0.2:    f=0.3:
â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€
          â”€â”€â”€â”€â”€
â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€
          â”€â”€â”€â”€â”€
â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€
```

**Ï† (Phase)**: Shifts stripes laterally
```
Ï†=0:      Ï†=Ï€/2:    Ï†=Ï€:
+â”€+       â”€+â”€       â”€+â”€
â”€+â”€       +â”€+       +â”€+
+â”€+       â”€+â”€       â”€+â”€
(+ = bright, - = dark)
```

**Ïƒx, Ïƒy (Sigmas)**: Control envelope shape
```
Ïƒx=Ïƒy:    Ïƒx>Ïƒy:    Ïƒx<Ïƒy:
  â—â—â—       â–¬â–¬â–¬       â–Œâ–Œâ–Œ
  â—â—â—       â–¬â–¬â–¬       â–Œâ–Œâ–Œ
  â—â—â—       â–¬â–¬â–¬       â–Œâ–Œâ–Œ
(isotropic) (wide)   (tall)
```

---

## Advanced Learnability Questions

### Q1: Can we make frequency ranges learnable per-filter?

**Current**: All filters share `fmin=0.05, fmax=0.25` (hardcoded in `__init__`)

**Proposal**: Make each filter have its own range
```python
# Instead of:
self.fmin, self.fmax = init_freq  # shared scalar

# Use:
self.fmin_u = nn.Parameter(torch.full([out_filters], init_freq[0]))
self.fmax_u = nn.Parameter(torch.full([out_filters], init_freq[1]))

# Then in _build_kernels:
fmin = torch.sigmoid(self.fmin_u) * 0.3  # [0, 0.3]
fmax = fmin + torch.sigmoid(self.fmax_u) * (0.5 - fmin)  # [fmin, 0.5]
freq = fmin + torch.sigmoid(self.freq_u) * (fmax - fmin)
```

**Benefits**:
- Some filters could specialize in low-freq (blobs), others in high-freq (fine texture)
- More adaptive to dataset statistics
- Total params: +2 per filter = +64 for 32 filters

**Drawbacks**:
- More parameters to tune
- Harder to interpret (loses uniform frequency bands)
- May need stronger regularization

---

### Q2: Can we make sigma ranges learnable?

**Current**: `smin = 3.0` (hardcoded floor), `smax = âˆ` (unbounded via softplus)

**Proposal**: Per-filter sigma bounds
```python
self.smin_u = nn.Parameter(torch.full([out_filters], 3.0))
self.smax_u = nn.Parameter(torch.full([out_filters], 8.0))

# In _build_kernels:
smin = F.softplus(self.smin_u) + 1.0  # minimum 1 pixel
smax = smin + F.softplus(self.smax_u)  # max > min
sigx = smin + torch.sigmoid(self.sigx_u) * (smax - smin)
```

**Benefits**:
- Filters could learn ultra-local (smin=1) vs global (smax=20) receptive fields
- Adaptive to image resolution

**Drawbacks**:
- Risk of collapse: sminâ†’0 kills filter, smaxâ†’âˆ makes it blob
- Need careful initialization and bounds

---

### Q3: Can we make kernel size adaptive per-filter?

**Current**: All filters share same `kernel_size=31` (spatial support)

**This is HARD** because:
1. PyTorch `F.conv2d` requires all kernels have same spatial size
2. Would need custom CUDA kernel or padding tricks
3. Dynamic shapes break batching efficiency

**Workaround**: Use sigma to control *effective* receptive field
- Large Ïƒ â†’ filter is smooth, ignores small kernel size
- Small Ïƒ â†’ filter is sharp, uses full 31Ã—31

So we **already have adaptive RF via Ïƒ**! No need for variable kernel size.

---

### Q4: Can we learn separate X and Y frequencies?

**Current**: Single frequency `f` applied along rotated X-axis only (standard Gabor)

**Proposal**: 2D frequency vector
```python
self.freqx_u = nn.Parameter(torch.zeros(out_filters))
self.freqy_u = nn.Parameter(torch.zeros(out_filters))

carrier = cos(2*Ï€ * (fx*xp + fy*yp) + Ï†)
```

**Result**: Creates **plaid patterns** (checkerboards) instead of gratings

**Biology**: Some V2 neurons respond to plaids, but V1 simple cells are 1D gratings

**Decision**: Stick with 1D frequency (standard Gabor) for V1 modeling

---

### Q5: Can we learn asymmetric phases?

**Current**: Single phase `Ï†` for cosine carrier

**Proposal**: Learn sin/cos mixture (complex Gabor)
```python
self.phase_cos_u = nn.Parameter(torch.zeros(out_filters))
self.phase_sin_u = nn.Parameter(torch.zeros(out_filters))

carrier_cos = cos(2*Ï€*f*xp)
carrier_sin = sin(2*Ï€*f*xp)
carrier = phase_cos * carrier_cos + phase_sin * carrier_sin
```

**This is equivalent** to current phase shift, just different parameterization

**Decision**: Keep current (simpler, interpretable as "phase angle")

---

## Critical Implementation Details

### Why Normalization Matters

```python
# Step 7: Zero-mean, unit L2
k = k - k.mean(dim=(1,2), keepdim=True)
k = k / (k.norm(dim=(1,2)) + Îµ)
```

**Without zero-mean**: DC component â†’ uniform brightness shift (not edge detection)
**Without unit norm**: Filters with high freq or large Ïƒ would dominate gradients

### Why We Use Unconstrained Parameters

```python
self.theta_u = nn.Parameter(torch.rand(out_filters) * 2*Ï€)  # NOT constrained!
```

**Problem**: If we used `theta = sigmoid(theta_u) * 2Ï€`, gradients vanish at boundaries

**Solution**: Use unconstrained â„ â†’ apply modulo in forward pass
- Gradients always flow (no saturation)
- Multiple rotations OK: Î¸=0 â‰¡ Î¸=2Ï€ â‰¡ Î¸=4Ï€

### Spatial Attention Mechanism

```python
spatial_attn = Sequential(
    Conv2d(N, N, 3, groups=N),  # depthwise (per-filter)
    ReLU(),
    Conv2d(N, N, 1),             # pointwise (cross-filter mixing)
    Sigmoid()                    # â†’ [0,1] attention map
)
y = gabor_maps * attention_maps  # element-wise modulation
```

**Purpose**: Let filters focus on informative regions
- Depthwise: Aggregates local context around each Gabor response
- Pointwise: Allows filter interactions (e.g., "strong horizontal AND weak vertical")
- Per-location weighting: Different weights at each (h,w) pixel

---

## Sparsity Mechanisms

### v1: L1 Penalty (Broken)

```python
sparsity_loss = sigmoid(gate_u).abs().mean()
```

**Problem**: `sigmoid(x) âˆˆ [0,1]`, always positive â†’ `abs()` does nothing!
**Result**: Penalty = mean(gate), encourages gatesâ†’0 uniformly (not selective)
**Observed**: Gates stuck at 0.82 (equilibrium where CE loss = sparsity pressure)

### v2: Entropy Penalty (Fixed)

```python
g = sigmoid(gate_u)
sparsity_loss = (g * (1-g)).mean() * 4
```

**Intuition**: Binary entropy $H = -p\log p - (1-p)\log(1-p)$, linearized as $4p(1-p)$

| Gate Value | Entropy Penalty | Interpretation |
|------------|----------------|----------------|
| g = 0.0 | 0.0 | âœ… Fully OFF (no penalty) |
| g = 0.5 | 1.0 | âŒ Uncertain (maximum penalty) |
| g = 1.0 | 0.0 | âœ… Fully ON (no penalty) |

**Result**: Encourages discrete decisions (0 or 1), not soft gates (0.5)

---

## Filter Interpretability

### What Makes Gabor Filters Interpretable?

1. **Explicit parameters**: Î¸=45Â° means "detects diagonal edges" (no blackbox)
2. **Biological correspondence**: Matches V1 simple cell tuning curves
3. **Visualizable**: Can plot learned kernels directly
4. **Sparse gating**: Can identify which filters are "active" vs "pruned"

### Example Learned Filter Analysis

```python
with torch.no_grad():
    theta = model.gabor.theta_u % (2*Ï€)
    freq = fmin + sigmoid(model.gabor.freq_u) * (fmax - fmin)
    gate = sigmoid(model.gabor.gate_u)
    
    for i in range(32):
        print(f"Filter {i}: Î¸={theta[i]*180/Ï€:.1f}Â°, "
              f"f={freq[i]:.3f}, gate={gate[i]:.2f}, "
              f"active={'âœ“' if gate[i]>0.5 else 'âœ—'}")
```

**Output example**:
```
Filter 0: Î¸=0.0Â°, f=0.150, gate=0.95, active=âœ“   (horizontal edges)
Filter 1: Î¸=45.0Â°, f=0.180, gate=0.89, active=âœ“  (diagonal edges)
Filter 2: Î¸=90.0Â°, f=0.120, gate=0.92, active=âœ“  (vertical edges)
Filter 3: Î¸=135.0Â°, f=0.200, gate=0.03, active=âœ— (pruned!)
...
```

---

## Potential Improvements

### 1. **Learnable Frequency Ranges** (Medium Priority)
- **Change**: Per-filter `fmin`, `fmax` instead of shared
- **Benefit**: Some filters specialize in low-freq (shapes), others high-freq (texture)
- **Cost**: +2 params/filter (+64 for 32 filters)
- **Implementation**: 10 lines of code

### 2. **Learnable Sigma Bounds** (Low Priority)
- **Change**: Per-filter `smin`, `smax`
- **Benefit**: Ultra-local vs global receptive fields
- **Risk**: Instability (filters could collapse to Ïƒâ†’0)
- **Alternative**: Current design already allows Ïƒ adaptation via softplus

### 3. **Phase Diversity Initialization** (High Priority)
- **Current**: `phase_u` initialized to zero (all cosine)
- **Better**: Initialize to [0, Ï€/2, Ï€, 3Ï€/2] for even/odd symmetry diversity
- **Benefit**: Faster convergence, better edge detection (complementary phases)
- **Implementation**: 2 lines in `__init__`

### 4. **Group-wise Frequency Bands** (Medium Priority)
- **Idea**: Divide 32 filters into 4 groups with non-overlapping frequency ranges
  - Group 1: [0.05, 0.10] - low-freq (blobs, shapes)
  - Group 2: [0.10, 0.15] - mid-low freq
  - Group 3: [0.15, 0.20] - mid-high freq
  - Group 4: [0.20, 0.25] - high-freq (fine texture)
- **Benefit**: Forced multi-scale representation
- **Implementation**: Different `fmin`/`fmax` per group in `__init__`

### 5. **Differentiable Kernel Size** (Low Priority, Hard)
- **Current limitation**: All kernels 31Ã—31
- **Workaround**: Already handled via Ïƒ (effective RF adapts)
- **True dynamic size**: Requires custom CUDA kernel (not worth it)

---

## Conclusion

The Gabor module is **highly learnable** with 6-7 parameters per filter:
- âœ… **Orientation** (Î¸): Fully learnable, unbounded
- âœ… **Frequency** (f): Learnable within `[fmin, fmax]` bounds
- âœ… **Phase** (Ï†): Fully learnable, unbounded
- âœ… **Sigma X/Y** (Ïƒx, Ïƒy): Learnable with floor `smin`
- âœ… **Gate**: Learnable [0,1] activation strength
- âš ï¸ **Gain** (v1 only): Learnable [-1,1] amplitude (removed in v2 for simplicity)

**Future work** could explore:
1. Per-filter frequency ranges (easy, useful)
2. Phase diversity initialization (easy, recommended)
3. Grouped frequency bands (medium difficulty, good for multi-scale)

The current design already provides **adaptive receptive fields** through Ïƒ learning, making additional kernel size flexibility unnecessary.
